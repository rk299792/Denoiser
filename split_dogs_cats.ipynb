{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ef0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2e1a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##List out the file names of CAT and DOG folder\n",
    "\n",
    "catpath=\"../CATS_DOGS/CAT\"\n",
    "\n",
    "cat_list=[]\n",
    "\n",
    "for folder, subfolder, files in os.walk(catpath):\n",
    "    for file in files:\n",
    "        cat_list.append(folder+\"/\"+file)\n",
    "\n",
    "        \n",
    "dogpath=\"../CATS_DOGS/DOG\"\n",
    "\n",
    "dog_list=[]\n",
    "\n",
    "for folder,subfolder, files in os.walk(dogpath):\n",
    "    for file in files:\n",
    "        dog_list.append(folder+\"/\"+file)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6866908",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Select images which fits our image size needs\n",
    "\n",
    "catim_sizes=[]\n",
    "cat_rejected=[]\n",
    "selected_catfiles=[]\n",
    "for item in cat_list:\n",
    "    try:\n",
    "        with Image.open(item) as im:\n",
    "           \n",
    "            \n",
    "            if (im.size[0]>=323 and im.size[1]>=302 and len(im.mode)==3):\n",
    "                catim_sizes.append(im.size)\n",
    "                selected_catfiles.append(item)\n",
    "    except:\n",
    "        cat_rejected.append(im)\n",
    "        \n",
    "        \n",
    "\n",
    "dogim_sizes=[]\n",
    "dog_rejected=[]\n",
    "selected_dogfiles=[]\n",
    "for item in dog_list:\n",
    "    try:\n",
    "        with Image.open(item) as im:\n",
    "           \n",
    "           \n",
    "            if (im.size[0]>=323 and im.size[1]>=302 and len(im.mode)==3):\n",
    "                dogim_sizes.append(im.size)\n",
    "                selected_dogfiles.append(item)\n",
    "    except:\n",
    "        dog_rejected.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd6c32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0            1\n",
      "count  8342.000000  8342.000000\n",
      "mean    459.131263   408.927116\n",
      "std      57.377488    60.372905\n",
      "min     323.000000   302.000000\n",
      "25%     411.000000   375.000000\n",
      "50%     500.000000   375.000000\n",
      "75%     500.000000   480.000000\n",
      "max     500.000000   500.000000\n",
      "                 0            1\n",
      "count  8556.000000  8556.000000\n",
      "mean    468.127279   402.019752\n",
      "std      52.378850    57.559483\n",
      "min     323.000000   302.000000\n",
      "25%     448.000000   375.000000\n",
      "50%     500.000000   375.000000\n",
      "75%     500.000000   450.000000\n",
      "max     500.000000   500.000000\n"
     ]
    }
   ],
   "source": [
    "##checking the image size statistics\n",
    "print(pd.DataFrame(dogim_sizes).describe())\n",
    "print(pd.DataFrame(catim_sizes).describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca9d9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the center crop transformation\n",
    "transform=transforms.Compose([transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5fffb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform the ground truth images and make the noisy image\n",
    "\n",
    "for item in selected_catfiles:\n",
    "    \n",
    "    gt=Image.open(item)\n",
    "\n",
    "    gt=np.array(transform(gt))\n",
    "\n",
    "\n",
    "    gt1=Image.fromarray(gt)\n",
    "    gt1.save(\"../catData/gt/\"+os.path.basename(item))\n",
    "    noisy=gt+np.random.normal(0,40,(224,224,3))\n",
    "\n",
    "    noisy=np.clip(noisy,0,255).astype(np.uint8)\n",
    "\n",
    "    noisy=Image.fromarray(noisy)\n",
    "    noisy.save(\"../catData/noise/\"+os.path.basename(item))\n",
    "\n",
    "    \n",
    "    \n",
    "for item in selected_dogfiles:\n",
    "    \n",
    "    gt=Image.open(item)\n",
    "\n",
    "    gt=np.array(transform(gt))\n",
    "\n",
    "\n",
    "    gt1=Image.fromarray(gt)\n",
    "    gt1.save(\"../dogData/gt/\"+os.path.basename(item))\n",
    "    noisy=gt+np.random.normal(0,40,(224,224,3))\n",
    "\n",
    "    noisy=np.clip(noisy,0,255).astype(np.uint8)\n",
    "\n",
    "    noisy=Image.fromarray(noisy)\n",
    "    noisy.save(\"../dogData/noise/\"+os.path.basename(item))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c898e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "from model import RedCNN\n",
    "from customDataset import CatdogDataset\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756d827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "gtpath=\"../dogData/gt/\"\n",
    "gtfiles=[]\n",
    "for folder, subfolder, filenames in os.walk(gtpath):\n",
    "    for files in filenames:\n",
    "        gtfiles.append(files)\n",
    "        \n",
    "\n",
    "noisepath=\"../dogData/noise/\"\n",
    "noisefiles=[]\n",
    "for folder, subfolder, filenames in os.walk(noisepath):\n",
    "    for files in filenames:\n",
    "        noisefiles.append(files)\n",
    "        \n",
    "#check whether both noise and gt have same files\n",
    "print(gtfiles==noisefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6d5c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dogdataset= CatdogDataset(data_path=noisepath,target_path=gtpath,filenames=gtfiles)\n",
    "train_size=int(0.7*len(dogdataset))\n",
    "test_size=len(dogdataset)-train_size\n",
    "trainset,testset=random_split(dogdataset,[train_size,test_size],generator=torch.manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7037d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RedCNN(\n",
      "  (conv1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (d_conv1): ConvTranspose2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (d_conv2): ConvTranspose2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (d_conv3): ConvTranspose2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (d_conv4): ConvTranspose2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (d_output): ConvTranspose2d(96, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batchnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_dog=RedCNN()\n",
    "model_dog.load_state_dict(torch.load(\"trained_on_cat_130batch.pth.tar\"))\n",
    "print(model_dog)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_dog.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6644ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss:252.43943786621094 Batch:0\n",
      "Epoch: 1 Loss:273.6606140136719 Batch:10\n",
      "Epoch: 1 Loss:265.46087646484375 Batch:20\n",
      "Epoch: 1 Loss:274.1610107421875 Batch:30\n",
      "Epoch: 1 Loss:263.66766357421875 Batch:40\n",
      "Epoch: 1 Loss:251.27737426757812 Batch:50\n",
      "Epoch: 1 Loss:264.0395202636719 Batch:60\n",
      "Epoch: 1 Loss:281.7471618652344 Batch:70\n",
      "Epoch: 1 Loss:257.4654846191406 Batch:80\n",
      "Epoch: 1 Loss:245.90756225585938 Batch:90\n",
      "Epoch: 1 Loss:270.5928039550781 Batch:100\n",
      "Epoch: 1 Loss:258.30633544921875 Batch:110\n",
      "Epoch: 1 Loss:269.86566162109375 Batch:120\n",
      "Epoch: 1 Loss:265.7371520996094 Batch:130\n",
      "Epoch: 1 Loss:269.8819580078125 Batch:140\n",
      "Epoch: 1 Loss:259.597900390625 Batch:150\n",
      "Epoch: 1 Loss:250.15341186523438 Batch:160\n",
      "Epoch: 1 Loss:248.4051971435547 Batch:170\n",
      "Epoch: 1 Loss:252.6630096435547 Batch:180\n"
     ]
    }
   ],
   "source": [
    "##Training\n",
    "optimizer.zero_grad()\n",
    "ewc_lambda=0.1\n",
    "\n",
    "param_dict=torch.load(\"parameter_dict.pth.tar\")\n",
    "fisher_dict=torch.load(\"fisher_dict.pth.tar\")\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "losses_batch=[]\n",
    "trained_psnr=[]\n",
    "for i, data in enumerate(train_loader):\n",
    "\n",
    "    inputt, target=data\n",
    "\n",
    "    target_pred=model_dog(inputt)\n",
    "\n",
    "    img=inputt.detach().numpy()\n",
    "    recon_image=target_pred.detach().numpy()\n",
    "    for j in range(len(target)):\n",
    "        trained_psnr.append(cv2.PSNR(img[j],recon_image[j]))\n",
    "\n",
    "    loss=criterion(target_pred,target)\n",
    "\n",
    "    for name, param in model_dog.named_parameters():\n",
    "        fisher=fisher_dict[name]\n",
    "        theta_star=param_dict[name]\n",
    "        loss+=(fisher*(param-theta_star).pow(2)).sum()*ewc_lambda\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    losses_batch.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i%10==0):\n",
    "        print(f\"Epoch: 1 Loss:{loss.item()} Batch:{i}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3947f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fisher_dict={}\n",
    "# param_dict={}\n",
    "            \n",
    "# for name, param in model.named_parameters():\n",
    "#     param_dict[name] = param.data.clone()\n",
    "#     fisher_dict[name] = param.grad.data.clone().pow(2)\n",
    "\n",
    "# param_dictfile=\"parameter_dict.pth.tar\"\n",
    "# fisher_dictfile=\"fisher_dict.pth.tar\"\n",
    "\n",
    "# torch.save(fisher_dict,fisher_dictfile)\n",
    "# torch.save(param_dict,param_dictfile)\n",
    "modelfile=\"trained_on_dogwithewc.pth.tar\"\n",
    "\n",
    "torch.save(model_dog.state_dict(),modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2509543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=RedCNN()\n",
    "\n",
    "# model.load_state_dict(torch.load(modelfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad26b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_psnr=[]\n",
    "# train_psnr=[]\n",
    "# for i in range(len(trainset)):\n",
    "#     noise,gt=trainset[i]\n",
    "#     gt_psnr.append(cv2.PSNR(noise.numpy(),gt.numpy()))\n",
    "#     train_psnr.append(cv2.PSNR(model(noise.view(-1,3,224,224)).detach().numpy().reshape(3,224,224),gt.numpy()))\n",
    "    \n",
    "# print(f\"Ground Truth PSNR(trainset): {sum(gt_psnr)/len(gt_psnr)}\")\n",
    "# print(f\"PSNR after training: {sum(trained_psnr)/len(trained_psnr)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
